{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "batch_size = 12\n",
    "epochs = 1\n",
    "model_max_length = 126\n",
    "label_list = ['O', 'B-c', 'I-c', 'B-p', 'I-p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "138\n",
      "[('ผญ.', 'NCMN', 'B-c'), (' ', 'PUNC', 'I-c'), ('เวน', 'NCMN', 'I-c'), ('ดี้', 'NCMN', 'I-c'), (' ', 'PUNC', 'I-c'), ('เพราะ', 'JSBR', 'B-p'), ('หน้า', 'NCMN', 'I-p'), ('สวย', 'NCMN', 'I-p'), ('ชอบ', 'VSTA', 'I-p'), ('ผญเเบบ', 'NCMN', 'I-p'), ('เวน', 'NCMN', 'I-p'), ('ดี้', 'NCMN', 'I-p'), ('เสียง', 'NCMN', 'I-p'), ('ใส', 'VATT', 'I-p'), ('พูด', 'ADVN', 'I-p'), ('อิ้ง', 'VACT', 'I-p'), ('เก่ง', 'NCMN', 'I-p'), ('มาก', 'ADVN', 'I-p'), (' ', 'PUNC', 'I-p'), (' ', 'PUNC', 'O'), ('ผช.', 'NCMN', 'B-c'), (' ', 'PUNC', 'I-c'), ('อยาก', 'XVMM', 'I-c'), ('เป็น', 'VSTA', 'I-c'), ('จี', 'NCMN', 'I-c'), ('มิ', 'NEG', 'I-c'), ('น', 'NCMN', 'I-c'), (' ', 'PUNC', 'I-c'), ('อยาก', 'XVMM', 'B-p'), ('เป็น', 'VSTA', 'I-p'), ('หนุ่ม', 'NCMN', 'I-p'), ('เ', 'NCMN', 'I-p'), ('เพ', 'NCMN', 'I-p'), ('รว', 'NCMN', 'I-p'), ('พราว', 'VSTA', 'I-p'), ('ให้', 'JSBR', 'I-p'), ('ผญ.', 'NCMN', 'I-p'), ('ใจ', 'NCMN', 'I-p'), ('ละลาย', 'NCMN', 'I-p'), ('เล่น', 'VACT', 'I-p'), ('ดู', 'VACT', 'I-p'), (' ', 'PUNC', 'I-p'), ('555555555', 'DCNM', 'I-p'), (' ', 'PUNC', 'I-p')]\n"
     ]
    }
   ],
   "source": [
    "path_name = \"../../dataset/data/\"\n",
    "\n",
    "with open(path_name + 'comment-pos.data', 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "tagged_sents = []\n",
    "for data in datatofile:\n",
    "    text_inside = []\n",
    "    for word, pos, label in data:\n",
    "        if word.strip() == '':\n",
    "            text_inside.append((' ', pos, label))\n",
    "        else:\n",
    "            text_inside.append((word, pos, label))\n",
    "    tagged_sents.append(text_inside)\n",
    "\n",
    "train_sents, test_sents = train_test_split(tagged_sents, test_size=0.2, random_state=42)\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n",
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dict(data):\n",
    "    datasets = dict()\n",
    "    datasets['id'] = []\n",
    "    datasets['tokens'] = []\n",
    "    datasets['pos_tags'] = []\n",
    "    datasets['ner_tags'] = []\n",
    "    for i, sent in enumerate(data):\n",
    "        _word = []\n",
    "        _label = []\n",
    "        _pos = []\n",
    "        for word, pos, label in sent:\n",
    "            _word.append(word)\n",
    "            _label.append(label)\n",
    "            _pos.append(pos)\n",
    "        datasets['id'].append(i)\n",
    "        datasets['tokens'].append(_word)\n",
    "        datasets['pos_tags'].append(_pos)\n",
    "        datasets['ner_tags'].append([label_list.index(l) for l in _label])\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:02:27.744582: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-21 15:02:27.744640: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 552\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset,Sequence, Features, Value, ClassLabel\n",
    "\n",
    "ft = Features({\n",
    "    'id': Value(\"int32\"),\n",
    "    'tokens': Sequence(Value(\"string\")), \n",
    "    'pos_tags': Sequence(Value(\"string\")),\n",
    "    'ner_tags': Sequence(ClassLabel(names=label_list))\n",
    "    })\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict(data_to_dict(train_sents), features=ft),\n",
    "    'test': Dataset.from_dict(data_to_dict(test_sents), features=ft)\n",
    "})\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 5\n",
      "['O', 'B-c', 'I-c', 'B-p', 'I-p']\n"
     ]
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "print('len:', len(label_list))\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>339</td>\n",
       "      <td>[ชอบ, ทำงาน, ที่, ออฟ, ฟิส,  , คอม, หน้าจอ, ใหญ่, กว่า,  , ป, ริ้น, งาน, มา, นั่ง, ดู, ได้, ด้วย,  , คุย, กับ, ทีม, ง่าย, กว่า, นั่ง, แชท,  , สรุป, งาน, เสร็จ, เร็ว, กว่า,  , ดีกว่า, เน็ต, กาก, ๆ, ที่, บ้าน,  , แถม, โต๊ะ, กับ, เก้าอี้นั่ง, แล้ว, ไม่, ปวดหลัง,  , (, จริงๆ, ก็, อยาก, ซื้อ, เก้าอี้, ดี, ๆ, นะ,  , แต่, ต้อง, ประหยัด, ตัง, )​, เลิกงาน, แล้วก็, เลิกงาน, จริง,  , wfh,  , บางที, มี, คน, โทร, หา, ทั้งที่, เรา, เลิกงาน, แล้ว,  , รู้สึก, เหมือน, เวลา, ส่วนตัว, กับ, เวลา, งาน, กลืนกัน, ไป, หมด]</td>\n",
       "      <td>[VSTA, VACT, RPRE, NCMN, NCMN, PUNC, NCMN, NCMN, VATT, JCMP, PUNC, NCMN, NCMN, NCMN, XVAE, VACT, VACT, XVAE, RPRE, PUNC, NCMN, RPRE, NCMN, VATT, JCMP, NCMN, DCNM, PUNC, VACT, NCMN, VSTA, ADVN, JCMP, PUNC, JSBR, NCMN, NCMN, NCMN, PREL, NCMN, PUNC, NCMN, NCMN, RPRE, NCMN, XVAE, NEG, VSTA, PUNC, PUNC, ADVI, JSBR, NCMN, VACT, NCMN, VATT, PUNC, NCMN, PUNC, JCRG, XVMM, VACT, NCMN, NCMN, NCMN, JCRG, NCMN, VATT, PUNC, NCMN, PUNC, NCMN, VSTA, NCMN, NCMN, VACT, PREL, PPRS, VACT, XVAE, PUNC, NCMN, VSTA, NCMN, VSTA, RPRE, NCMN, NCMN, VSTA, XVAE, ADVN]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>492</td>\n",
       "      <td>[ภาค, ตะวันออก,  ,  , เพราะ, มี, พี่, อยู่, จว,  , ตราด,  , หน้าบ้าน, เป็น, ถนน, ใน, ซอยตัน, ลึก, 200,  , เมตร, เท่านั้น,  , หลังบ้าน, เป็น, แม่น้ำ, ตราด,  , อยู่, ไกล, จาก, ตลาด, ใน, ตัว, จังหวัด, 2,  , กม.,  , สงบ,  , อาหาร, อุดม, สม, บูร, ์, ทั้ง, ทะเล, จาก, อ่าวไทย,  , พืช, ผลไม้, จาก, สวน,  , ตลอดจน, เกาะ,  , น้ำตก,  , อากาศ, ไม่, ร้อน, ตับ, แตก, แบบ, เหนือ, คห.,  , ข้างบน, บอก,  , และ, ไม่, ฝนตก, ทั้งปี, แบบ, ภาคใต้,  , มี, สาม, ฤดู,  , หน้าหนาว, มี,  , หน้าฝน, ก็, ใช่,  , ชาว, สวนผลไม้, มี, เจอ, แล้ง, บาง, ปี,  ]</td>\n",
       "      <td>[NCMN, NCMN, PUNC, PUNC, JSBR, VSTA, NCMN, XVAE, NCMN, PUNC, NCMN, PUNC, NCMN, VSTA, NCMN, RPRE, NCMN, VATT, DCNM, PUNC, CMTR, ADVN, PUNC, NCMN, VSTA, NCMN, NCMN, NCMN, VSTA, ADVN, RPRE, NCMN, RPRE, NCMN, NCMN, DCNM, PUNC, CMTR, PUNC, NCMN, PUNC, NCMN, NCMN, NCMN, NCMN, NCMN, DDBQ, NCMN, RPRE, NPRP, PUNC, NCMN, NCMN, RPRE, NCMN, PUNC, JCRG, NCMN, PUNC, NCMN, NCMN, NCMN, NEG, VATT, NCMN, NCMN, NCMN, RPRE, NCMN, PUNC, NCMN, VACT, PUNC, JCRG, NEG, VACT, NCMN, NCMN, NCMN, PUNC, VSTA, DCNM, NCMN, PUNC, NCMN, VSTA, PUNC, NCMN, JSBR, VSTA, PUNC, NCMN, NCMN, VSTA, NCMN, NCMN, DIBQ, CMTR, PUNC]</td>\n",
       "      <td>[B-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>[ตอบ, ได้, คำ, เดียว, สั้น, ๆ, ง่ายๆ,  , จริง, แท้, แน่นอน, คะ,  , ไม่, ควร, ให้, ลูก, ดู, ทีวี,  , เล่น, ไอ, เพ, ด,  , จนกว่า, อายุ, จะ, ครบ, 2, ขวบ,  , มี, งานวิจัย, ออกมา, แล้ว, มากมาย, คะ,  , ว่าการ, ดู, ทีวี, จะ, ทำให้, เด็ก, สมาธิ, สั้น, จิงๆ, ค่า]</td>\n",
       "      <td>[VACT, XVAE, NCMN, DCNM, JSBR, PUNC, NCMN, PUNC, VATT, CNIT, VATT, NCMN, PUNC, NEG, XVMM, VACT, NCMN, VACT, NCMN, PUNC, VACT, NCMN, NCMN, NCMN, PUNC, JSBR, NCMN, XVBM, VSTA, NCNM, NCMN, PUNC, VSTA, NCMN, XVAE, XVAE, ADVN, NCMN, PUNC, NCMN, VACT, NCMN, XVBM, VACT, NCMN, NCMN, VATT, VACT, NCMN]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>[ถ้า, คุณ, ​, เรียนจบ, หา, แนวทาง, ทำงาน, ต่างประเทศ, ​, ได้, ถือว่า, คุ้ม, มาก, ๆ, ค่ะ, ​,  , วิศวะ, ถือ, เป็นงาน,  , special, ​, ist,  , ใน, หลาย, ประเทศ, ​, ยัง, ต้องการ, อยู่,  , ทั้งนี้, ​, คุณ, ​, ต้อง, เก่ง, ใน, สาย, งาย, จริงๆ, ด้วย,  , มากกว่า, นั้น, คือ, ประสบการณ์, ​, ชีวิต, ​, ที่, มีค่า, มากกว่า, เงิน, ค่ะ, ​,  , แต่, ต้อง, ไม่, เรียน, ออนไลน์, ​, นะคะ, ​,  , กลัว, โควิด, ไม่, จบ, เรียน, ผ่าน, คอม, แทน]</td>\n",
       "      <td>[JSBR, VACT, NCMN, NCMN, VACT, NCMN, VACT, NCMN, NCMN, XVAE, JSBR, NCMN, ADVN, NCMN, NCMN, NCMN, PUNC, NCMN, VSTA, NCMN, PUNC, NCMN, NCMN, NCMN, PUNC, RPRE, DIBQ, CNIT, JCRG, XVBM, VSTA, VSTA, NCMN, JCRG, NCMN, NCMN, NCMN, XVMM, VACT, RPRE, NCMN, NCMN, ADVI, RPRE, PUNC, NCMN, DDAC, VSTA, NCMN, NCMN, NCMN, NCMN, PREL, VSTA, JCMP, NCMN, NCMN, NCMN, PUNC, JCRG, XVMM, NEG, VACT, NCMN, NCMN, NCMN, NCMN, PUNC, NCMN, NCMN, NEG, VSTA, VACT, VSTA, NCMN, VSTA]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417</td>\n",
       "      <td>[ใช้, แบบ, ครีม, ของ, ลา, โร, ซ, ค่ะ,  ,  , เพราะ, แบบ, สเปรย์, รู้สึก, มัน, ฟุ้ง, ๆ,  , ดีไม่ดี, เรา, อาจจะ, หายใจ, เข้าไป, อีก,  , พ่น, ไม่, ดี, ก็, เลอะ, พื้น,  , เลย, เลือก, แบบ, ครีม, ทา, เอา, ค่ะ,  ]</td>\n",
       "      <td>[VACT, NCMN, NCMN, RPRE, NCMN, NCMN, NCMN, NCMN, PUNC, PUNC, JSBR, NCMN, NCMN, NCMN, PPRS, VATT, PUNC, PUNC, VACT, PPRS, XVMM, VACT, XVAE, DDBQ, PUNC, NCMN, NEG, VATT, JSBR, VSTA, NCMN, PUNC, XVAE, VACT, NCMN, NCMN, NCMN, VACT, NCMN, PUNC]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>512</td>\n",
       "      <td>[คิด, ว่า, น่าจะ, เอา, บิท, คอย, ลง, ยาก,  , เพราะ, ต่อให้, แต่ละ, ประเทศ, มี, ค, ริ, ป, โต, เป็น, ของ, ตัวเอง, .,  , มัน, ก็, จะ, ยัง, ต่าง, กับ, บิท, คอย,  , ที่, ไม่, มี, สัญชาติ,  , และ, ไม่, มี, ใคร, ควบคุม, ได้, อยู่ดี,  , ตราบใดที่, ตี, ข้อดี, ข้อ, นี้, ลง, ไม่, ได้,  , ก็, เอา, บิท, คอย, ให้, หาย, ไป, จาก, ระบบ, ไม่, ได้, ครับ,  , ส่วน, เรื่อง, เก็งกำไร, นั่น, อีก, เรื่อง]</td>\n",
       "      <td>[VACT, JSBR, XVMM, VACT, NCMN, VACT, XVAE, ADVN, PUNC, JSBR, VACT, DIBQ, CNIT, VSTA, NCMN, NCMN, NCMN, NCMN, VSTA, RPRE, PDMN, PUNC, NCMN, PPRS, JSBR, XVBM, XVBM, VSTA, RPRE, NCMN, VACT, PUNC, PREL, NEG, VSTA, NCMN, PUNC, JCRG, NEG, VSTA, PNTR, VACT, XVAE, ADVP, PUNC, JSBR, VACT, NCMN, CNIT, DDAC, XVAE, NEG, XVAE, PUNC, JSBR, VACT, NCMN, VACT, JSBR, VSTA, XVAE, RPRE, NCMN, NEG, XVAM, VACT, PUNC, NCMN, NCMN, NCMN, PDMN, DDBQ, NCMN]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>534</td>\n",
       "      <td>[เรื่อง, จอ, น่าจะ, เหมือน,  , Transformer, 5,  , มั้ง, เดา, เอา,  , เพราะ, เรื่อง, นี้, มัน, มี, ฉาก, ที่, ถ่าย, ด้วย, กล้อง,  , IMAX,  , ด้วย,  , ซึ่ง, เจ้า, กล้อง,  , IMAX,  , เนี่ย,  , มัน, จะ, เห็น, ซ้าย, ขวา, เท่า, ปกติ, กับ, เรื่อง, อื่น,  , แต่, มัน, ไป, ขยาย,  , บน, -, ล่าง,  , เพราะฉะนั้น, เดา, ว่า, ส่วน, ที่, มัน, ติด, แถบ, ดำ,  , มัน, ควรจะ, มี, บาง, ฉาก, ที่, แถบ, ดำ, หาย, ไป,  , หรือ, มี, น้อยลง,  , เพราะ, ภาพ, จาก, กล้อง,  , IMAX,  , จะ, ไป, เพิ่ม, พื่น, ที่, ตรงนั้น,  , ซึ่ง,  , Transformer, 5,  , ก็, ...]</td>\n",
       "      <td>[NCMN, NCMN, XVMM, VSTA, PUNC, NCMN, NCNM, PUNC, JCRG, NCMN, VACT, PUNC, JSBR, NCMN, DDAC, PPRS, VSTA, NCMN, PREL, VACT, RPRE, NCMN, PUNC, NCMN, PUNC, RPRE, PUNC, JSBR, VACT, NCMN, PUNC, NCMN, PUNC, VACT, PUNC, PPRS, XVBM, VSTA, NCMN, NCMN, VSTA, VATT, RPRE, NCMN, DIAC, PUNC, JCRG, PPRS, VACT, VACT, PUNC, RPRE, PUNC, VATT, PUNC, JSBR, VSTA, JSBR, NCMN, PREL, PPRS, VACT, NCMN, VATT, PUNC, PPRS, XVMM, VSTA, DIBQ, CNIT, PREL, NCMN, VATT, VSTA, XVAE, PUNC, JCRG, VSTA, ADVN, PUNC, JSBR, NCMN, RPRE, NCMN, PUNC, NCMN, PUNC, XVBM, VACT, VACT, NCMN, PREL, DDAC, PUNC, JSBR, PUNC, NCMN, NCNM, PUNC, JSBR, ...]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>[ไม่, เห็นด้วย, ครับ,  , ไม่งั้น, บางคน, มั่ว, ท้อง,  , มั่ว, ท้อง,  , ทำแท้ง, เป็น,  , 100,  , ครั้ง,  , ทำ, ยังไง,  , เฮ้อ, ชีวิต,  , และ, เด็ก, ที่, ถูก, ทิ้ง, หรือไม่, มี, พ่อแม่, ดูแล,  , มัน, ก็, ไม่, ได้, ชี้ชัด, ว่า, เค้า, จะ, เป็น, คน, ไม่, ดี,  , บางคน, ก็, มี, อนาคต, ที่, ดี, เยอะแยะ, ไป,  , เรา, ไม่, ควร, ไป, ตัดสิน, ชีวิต, เค้า,  , เค้า, เกิด, มา, แล้วก็, ควร, มี, สิทธิ, ได้, เกิด, มา]</td>\n",
       "      <td>[NEG, VSTA, NCMN, PUNC, NCMN, NCMN, VSTA, NCMN, NCMN, VSTA, NCMN, PUNC, NCMN, VSTA, PUNC, DCNM, PUNC, CFQC, PUNC, VACT, NCMN, PUNC, NCMN, NCMN, NCMN, JCRG, NCMN, PREL, XVAM, VACT, EITT, VSTA, NCMN, VACT, NCMN, PPRS, JSBR, NEG, XVAM, VACT, JSBR, NCMN, XVBM, VSTA, NCMN, NEG, VATT, JCMP, NCMN, JSBR, VSTA, NCMN, PREL, VATT, ADVN, XVAE, PUNC, PPRS, NEG, XVMM, VACT, VACT, NCMN, NCMN, PUNC, NCMN, VSTA, XVAE, JCRG, XVMM, VSTA, NCMN, XVAE, VSTA, XVAE]</td>\n",
       "      <td>[B-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, O, B-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>[ผู้ชาย,  , -,  , จอง, กุก, ค่ะ,  , เพราะ, หล่อ,  , ความสามารถ, เยอะ, เก่ง, หลายอย่าง,  ]</td>\n",
       "      <td>[NCMN, PUNC, PUNC, PUNC, VACT, NCMN, NCMN, PUNC, JSBR, NCMN, PUNC, NCMN, VACT, NCMN, NCMN, PUNC]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>[หลาย, ๆคน, จะ, มองว่า, เรา, ไร้, อนาคต,  , แก่ตัว, ไป, แรง, ก็, หมด,  , โดน, ไล่ออก,  , ไม่, แปลกที่, คน, จะ, มอง, แบบ, นั้น,  , เพราะ, ประเทศ, เรา, เป็น, ประเทศ, ที่, ให้, ค่า, กับ, อาชีพ, คน, ที่, ใช้, แรงงาน, น้อย, จน, เหมือน, พวก, เค้า, เป็น, คน, คนละชั้น, กับ, เรา,  , ทั้ง, ที่จริง, ทุก, อาชีพ, นั้น, สำคัญ, แต่, ด้วย, เรา, เลือก, เกิด, ไม่, ได้, ต้อง, มา, เกิด, ในประเทศ, นี้, ก็, ช่วยไม่ได้,  , เรา, เลย, วางแผน, ไว้, หมด, ตั้งแต่, อายุ, 18,  , ว่า, เรา, จะ, ทำ, อาชีพ, ใช้, แรงงาน, อย่างไร, ให้, รอด, และ, มีความสุข, ได้, จน, แก่]</td>\n",
       "      <td>[DIBQ, NCMN, XVBM, VSTA, PPRS, VSTA, NCMN, PUNC, VSTA, XVAE, VATT, JSBR, VSTA, PUNC, NCMN, NCMN, PUNC, NEG, VSTA, NCMN, XVBM, VACT, NCMN, DDAC, PUNC, JSBR, NCMN, PPRS, VSTA, NCMN, PREL, VACT, NCMN, RPRE, NCMN, NCMN, PREL, VACT, NCMN, ADVN, JSBR, VSTA, NCMN, NCMN, VSTA, NCMN, DDAC, RPRE, PPRS, PUNC, JCRG, ADVS, DIBQ, CNIT, DDAC, VATT, JCRG, RPRE, PPRS, VACT, VSTA, NEG, XVAE, XVMM, XVAE, VSTA, NCMN, DDAC, JSBR, VSTA, PUNC, PPRS, VACT, VACT, XVAE, ADVN, JSBR, NCMN, DCNM, PUNC, JSBR, PPRS, XVBM, VACT, NCMN, VACT, NCMN, PNTR, VACT, NCMN, JCRG, NCMN, XVAE, JSBR, RPRE]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer  \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เครื่องกล', ' ', ' ', 'เพราะ', 'หน้า', 'งาน', 'กว้าง', 'มาก', 'ๆ', ' ']\n"
     ]
    }
   ],
   "source": [
    "example = datasets[\"train\"][2]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁', 'เครื่อง', 'กล', '▁', '▁', '▁เพราะ', '▁', 'หน้า', '▁งาน', '▁', 'กว้าง', '▁', 'มาก', '▁', 'ๆ', '▁', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 18\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5, 391, 10, 91, 10, 961, 10, 10, 28, 10, 10, 81, 10, 3742, 3325, 10, 10, 16820, 10, 10, 5261, 10, 158, 10, 10, 10, 2817, 10, 2732, 10, 158, 10, 222, 10, 2817, 10, 822, 10, 158, 10, 10, 391, 1896, 10, 200, 10, 593, 10, 9396, 10, 28, 10, 10, 3, 10569, 1189, 10, 3310, 10, 3038, 10, 12677, 5678, 10, 294, 10, 1905, 10, 73, 6]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c39358f62fe4147a7d64eace604440d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7718d15bf634e36a1e9d6cc069c271f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 552\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'p': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: ner_tags, pos_tags, tokens, id. If ner_tags, pos_tags, tokens, id are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/mike/project/env_project_argument/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 552\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 46\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:02:55.392160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-21 15:02:55.392212: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084610</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.025806</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>0.549348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: ner_tags, pos_tags, tokens, id. If ner_tags, pos_tags, tokens, id are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 138\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to wangchanberta-base-att-spm-uncased-finetuned-ner/checkpoint-46\n",
      "Configuration saved in wangchanberta-base-att-spm-uncased-finetuned-ner/checkpoint-46/config.json\n",
      "Model weights saved in wangchanberta-base-att-spm-uncased-finetuned-ner/checkpoint-46/pytorch_model.bin\n",
      "tokenizer config file saved in wangchanberta-base-att-spm-uncased-finetuned-ner/checkpoint-46/tokenizer_config.json\n",
      "Special tokens file saved in wangchanberta-base-att-spm-uncased-finetuned-ner/checkpoint-46/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=46, training_loss=1.1841389199961787, metrics={'train_runtime': 42.1679, 'train_samples_per_second': 13.091, 'train_steps_per_second': 1.091, 'total_flos': 35496494036640.0, 'train_loss': 1.1841389199961787, 'epoch': 1.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: ner_tags, pos_tags, tokens, id. If ner_tags, pos_tags, tokens, id are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 138\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0846095085144043,\n",
       " 'eval_precision': 0.012269938650306749,\n",
       " 'eval_recall': 0.025806451612903226,\n",
       " 'eval_f1': 0.016632016632016633,\n",
       " 'eval_accuracy': 0.5493477027793534,\n",
       " 'eval_runtime': 2.4427,\n",
       " 'eval_samples_per_second': 56.495,\n",
       " 'eval_steps_per_second': 4.913,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: ner_tags, pos_tags, tokens, id. If ner_tags, pos_tags, tokens, id are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 138\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': {'precision': 0.02512562814070352,\n",
       "  'recall': 0.0364963503649635,\n",
       "  'f1': 0.029761904761904764,\n",
       "  'number': 274},\n",
       " 'p': {'precision': 0.0034482758620689655,\n",
       "  'recall': 0.010471204188481676,\n",
       "  'f1': 0.005188067444876784,\n",
       "  'number': 191},\n",
       " 'overall_precision': 0.012269938650306749,\n",
       " 'overall_recall': 0.025806451612903226,\n",
       " 'overall_f1': 0.016632016632016633,\n",
       " 'overall_accuracy': 0.5493477027793534}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁นี่', '▁', 'ไง', '▁', '▁ใคร', '▁', 'บอ', '▁', 'กว่า', '▁', 'ชื่อเสียง', '▁', 'มหาวิทยาลั', 'ย', '▁ไม่', '▁', 'สําคัญ', '▁', '▁สําหรับ', '▁เด็ก', '▁', 'จบ', '▁', 'ใหม่', '▁', '▁', 'สําคัญ', '▁', 'มาก', '▁', 'ครับ', '▁', '▁เคย', '▁', 'ทํางาน', '▁บริษัท', '▁', 'จัดหา', '▁งาน', '▁', '▁', 'ลูกค้า', '▁ขอ', '▁', 'มา', '▁', 'เลย', '▁', '▁', 'สายงาน', '▁', 'นี้', '▁ต้อง', '▁', 'มหาลัย', '▁', 'นี้', '▁', 'เท่านั้น', '▁', '▁เพราะ', '▁คุณ', '▁ไม่', '▁มี', 'ประสบการณ์', '▁', '▁', 'เลย', '▁ต้อง', '▁ใช้', '▁', 'มหาวิทยาลั', 'ย', '▁', 'มา', '▁', 'เป็น', '▁', 'เกณฑ์', '▁', 'ใน', '▁การ', '▁', 'ตัดสิน', '▁', '▁', 'อย่าง', '▁', 'น้อย', '▁', 'มหาวิทยาลั', 'ย', '▁', 'ที่', '▁มี', 'ชื่อเสียง', '▁', '▁ก็', '▁', 'วัด', '▁', 'ได้', '▁', 'ประมาณ', '▁', 'หนึ่ง', '</s>']\n"
     ]
    }
   ],
   "source": [
    "idx = 20\n",
    "show_text = tokenizer(tokenized_datasets[\"test\"][idx]['tokens'], is_split_into_words=True)\n",
    "print(tokenizer.convert_ids_to_tokens(show_text['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: ['I-c', 'I-c', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-c', 'I-c', 'I-c', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-c']\n",
      "true: ['B-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p']\n"
     ]
    }
   ],
   "source": [
    "print('pred:', true_predictions[idx])\n",
    "print('true:', true_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ฉัน', 'ชอบ', 'หมา', 'เพราะ', 'มัน', 'น่ารัก', 'มาก', 'ๆ', ' ', 'เลย']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "text_list = word_tokenize('ฉันชอบหมาเพราะมันน่ารักมากๆ เลย')\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 264, 1879, 10, 1022, 474, 661, 5840, 10, 82, 10, 34, 10, 10, 48, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁ฉัน', '▁ชอบ', '▁', 'หมา', '▁เพราะ', '▁มัน', '▁น่ารัก', '▁', 'มาก', '▁', 'ๆ', '▁', '▁', 'เลย', '</s>']\n"
     ]
    }
   ],
   "source": [
    "_input_token = tokenizer(text_list, is_split_into_words=True)\n",
    "_word_token = tokenizer.convert_ids_to_tokens(_input_token[\"input_ids\"])\n",
    "print(_input_token)\n",
    "print(_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = trainer.predict([_input_token])[0]\n",
    "pred = np.argmax(pred, axis=2)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-c', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p']\n"
     ]
    }
   ],
   "source": [
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(pred, labels)\n",
    "][0]\n",
    "print(true_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(true_predictions))\n",
    "print(len(_word_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('▁ฉัน', 'I-c')\n",
      "('▁ชอบ', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('หมา', 'I-p')\n",
      "('▁เพราะ', 'I-p')\n",
      "('▁มัน', 'I-p')\n",
      "('▁น่ารัก', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('มาก', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('ๆ', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('เลย', 'I-p')\n"
     ]
    }
   ],
   "source": [
    "for w, l in zip(_word_token[1:-1], true_predictions):\n",
    "  print((w, l))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "018e0a3ac4678c6eee4f5b6012f6866bd583f46fe819b31cdc8524b9233bdcf3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('wangchan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
