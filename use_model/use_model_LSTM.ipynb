{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55005bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / Load File\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "# Load Vectors\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Utility\n",
    "import numpy as np\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Keras Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input ,LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from tensorflow.keras.layers import concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "492c7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../trained_model/LSTM/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38139d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thai2fit_model = KeyedVectors.load_word2vec_format('../thai2vec/thai2vecNoSym.bin',binary=True)\n",
    "thai2fit_weight = thai2fit_model.vectors\n",
    "\n",
    "thai2dict = {}\n",
    "\n",
    "for word in thai2fit_model.index_to_key:\n",
    "    thai2dict[word] = thai2fit_model[word]\n",
    "\n",
    "all_thai2dict = sorted(set(thai2dict))\n",
    "thai2dict_to_ix = dict((c, i) for i, c in enumerate(thai2dict)) #convert thai2fit to index \n",
    "ix_to_thai2dict = dict((v,k) for k,v in thai2dict_to_ix.items())  #convert index to thai2fit\n",
    "\n",
    "n_thai2dict = len(thai2dict_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cb5663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path+'nerdict.pickle', 'rb') as nerdict:\n",
    "    ner_to_ix = pickle.load(nerdict)\n",
    "\n",
    "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items())  #convert index to ner\n",
    "n_tag = len(ner_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4033fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path+'chardict.pickle', 'rb') as chardict:\n",
    "    char2idx = pickle.load(chardict)\n",
    "\n",
    "n_chars = len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b69edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 400\n",
    "max_len_char = 32\n",
    "\n",
    "character_LSTM_unit = 32\n",
    "char_embedding_dim = 32\n",
    "main_lstm_unit = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d951a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_word(list_sent):\n",
    "    idxs = list()\n",
    "    for word in list_sent:\n",
    "        if word in thai2dict:\n",
    "            idxs.append(thai2dict_to_ix[word])\n",
    "        else:\n",
    "            idxs.append(thai2dict_to_ix[\"unknown\"]) #Use UNK tag for unknown word\n",
    "    return idxs\n",
    "\n",
    "def prepare_sequence_target(input_label):\n",
    "    idxs = [ner_to_idx[BIO] for BIO in input_label]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c2465e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word Input\n",
    "word_in = Input(shape=(max_len), name='word_input_')\n",
    "\n",
    "#word Enbedding Using Thai2Fit\n",
    "word_embeddings = Embedding(input_dim=n_thai2dict, output_dim=400, weights = [thai2fit_weight], input_length=max_len,\n",
    "                                               mask_zero=False, trainable=False, name=\"word_embedding\")(word_in)\n",
    "\n",
    "# Character Input\n",
    "char_in = Input(shape=(max_len, max_len_char,), name='char_input')\n",
    "\n",
    "# Character Embedding\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=char_embedding_dim, \n",
    "                           input_length=max_len_char, mask_zero=False))(char_in)\n",
    "\n",
    "# Character Sequence to Vector via BiLSTM\n",
    "char_enc = TimeDistributed(LSTM(units=character_LSTM_unit, return_sequences=False))(emb_char)\n",
    "\n",
    "# Concatenate All Embedding\n",
    "all_word_embeddings = concatenate([word_embeddings, char_enc])\n",
    "all_word_embeddings = SpatialDropout1D(0.3)(all_word_embeddings)\n",
    "\n",
    "main_lstm = LSTM(units=main_lstm_unit, return_sequences=True,)(all_word_embeddings)\n",
    "dens = TimeDistributed(Dense(100, activation=\"relu\"))(main_lstm)\n",
    "out = Dense(n_tag, activation=\"softmax\")(dens)\n",
    "model = keras.Model(inputs=[word_in, char_in], outputs=[out])\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88a75dde",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mike/project/project-argument-tagger/use_model/use_model_LSTM.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mike/project/project-argument-tagger/use_model/use_model_LSTM.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_weights(model_path\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel_LSTM.hdf5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/project/env_project_argument/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/project/env_project_argument/lib/python3.8/site-packages/keras/engine/training.py:2605\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/engine/training.py?line=2601'>2602</a>\u001b[0m       hdf5_format\u001b[39m.\u001b[39mload_weights_from_hdf5_group_by_name(\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/engine/training.py?line=2602'>2603</a>\u001b[0m           f, \u001b[39mself\u001b[39m, skip_mismatch)\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/engine/training.py?line=2603'>2604</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/engine/training.py?line=2604'>2605</a>\u001b[0m       hdf5_format\u001b[39m.\u001b[39;49mload_weights_from_hdf5_group(f, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/engine/training.py?line=2606'>2607</a>\u001b[0m \u001b[39m# Perform any layer defined finalization of the layer state.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/engine/training.py?line=2607'>2608</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n",
      "File \u001b[0;32m~/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py:740\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=737'>738</a>\u001b[0m layer \u001b[39m=\u001b[39m filtered_layers[k]\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=738'>739</a>\u001b[0m symbolic_weights \u001b[39m=\u001b[39m _legacy_weights(layer)\n\u001b[0;32m--> <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=739'>740</a>\u001b[0m weight_values \u001b[39m=\u001b[39m load_subset_weights_from_hdf5_group(g)\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=740'>741</a>\u001b[0m weight_values \u001b[39m=\u001b[39m preprocess_weights_for_loading(layer, weight_values,\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=741'>742</a>\u001b[0m                                                original_keras_version,\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=742'>743</a>\u001b[0m                                                original_backend)\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=743'>744</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(weight_values) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(symbolic_weights):\n",
      "File \u001b[0;32m~/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py:686\u001b[0m, in \u001b[0;36mload_subset_weights_from_hdf5_group\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=672'>673</a>\u001b[0m \u001b[39m\"\"\"Load layer weights of a model from hdf5.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=673'>674</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=674'>675</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=682'>683</a>\u001b[0m \u001b[39m        and weights file.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=683'>684</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=684'>685</a>\u001b[0m weight_names \u001b[39m=\u001b[39m load_attributes_from_hdf5_group(f, \u001b[39m'\u001b[39m\u001b[39mweight_names\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=685'>686</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [np\u001b[39m.\u001b[39masarray(f[weight_name]) \u001b[39mfor\u001b[39;00m weight_name \u001b[39min\u001b[39;00m weight_names]\n",
      "File \u001b[0;32m~/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py:686\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=672'>673</a>\u001b[0m \u001b[39m\"\"\"Load layer weights of a model from hdf5.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=673'>674</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=674'>675</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=682'>683</a>\u001b[0m \u001b[39m        and weights file.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=683'>684</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=684'>685</a>\u001b[0m weight_names \u001b[39m=\u001b[39m load_attributes_from_hdf5_group(f, \u001b[39m'\u001b[39m\u001b[39mweight_names\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/keras/saving/hdf5_format.py?line=685'>686</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [np\u001b[39m.\u001b[39;49masarray(f[weight_name]) \u001b[39mfor\u001b[39;00m weight_name \u001b[39min\u001b[39;00m weight_names]\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py:1015\u001b[0m, in \u001b[0;36mDataset.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py?line=1011'>1012</a>\u001b[0m \u001b[39mif\u001b[39;00m numpy\u001b[39m.\u001b[39mproduct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mulonglong) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py?line=1012'>1013</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n\u001b[0;32m-> <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py?line=1014'>1015</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_direct(arr)\n\u001b[1;32m   <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py?line=1015'>1016</a>\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py:976\u001b[0m, in \u001b[0;36mDataset.read_direct\u001b[0;34m(self, dest, source_sel, dest_sel)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py?line=972'>973</a>\u001b[0m     dest_sel \u001b[39m=\u001b[39m sel\u001b[39m.\u001b[39mselect(dest\u001b[39m.\u001b[39mshape, dest_sel)\n\u001b[1;32m    <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py?line=974'>975</a>\u001b[0m \u001b[39mfor\u001b[39;00m mspace \u001b[39min\u001b[39;00m dest_sel\u001b[39m.\u001b[39mbroadcast(source_sel\u001b[39m.\u001b[39marray_shape):\n\u001b[0;32m--> <a href='file:///home/mike/project/env_project_argument/lib/python3.8/site-packages/h5py/_hl/dataset.py?line=975'>976</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid\u001b[39m.\u001b[39;49mread(mspace, fspace, dest, dxpl\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dxpl)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_weights(model_path+\"model_LSTM.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb789e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word_to_char(predict_word):\n",
    "    predict_char = []\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):    \n",
    "            try:\n",
    "                if(predict_word[i][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(predict_word[i][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    predict_char.append(np.array(sent_seq))\n",
    "    \n",
    "    return predict_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a247c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_html_format2(pred_list):\n",
    "    LIST_TAGS = ['claim', 'premise', 'o']\n",
    "    REPRESEN_TAGS = ['c', 'p', 'O']\n",
    "    text_convert = ''\n",
    "    prev_tag = ''\n",
    "    trigger_tag = False \n",
    "\n",
    "\n",
    "    for word, label in pred_list:\n",
    "        tags = label.split('-')\n",
    "        next_tag = tags[0] if len(tags) == 1 else tags[1]\n",
    "        \n",
    "        if prev_tag != next_tag:\n",
    "            if prev_tag:\n",
    "                text_convert += '</' + html_tag + '>'\n",
    "\n",
    "            html_tag = LIST_TAGS[REPRESEN_TAGS.index(next_tag)]\n",
    "            prev_tag = next_tag\n",
    "            trigger_tag = not(trigger_tag)\n",
    "\n",
    "            if trigger_tag:\n",
    "                text_convert += '<' + html_tag + '>'\n",
    "            else:\n",
    "                text_convert += '<' + html_tag + '>'\n",
    "\n",
    "        text_convert += word\n",
    "    text_convert += '</'+ LIST_TAGS[REPRESEN_TAGS.index(prev_tag)]+'>'\n",
    "    text_convert = text_convert.replace('<o>', '').replace('</o>', '')\n",
    "            \n",
    "    return text_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4483cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocess_text(text, token=True):\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    if token == True:\n",
    "        text = word_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text):\n",
    "    predict_sent = prepocess_text(text)\n",
    "    len_word = len(predict_sent)\n",
    "    predict_word = []\n",
    "    predict_word = [prepare_sequence_word(predict_sent)]\n",
    "    predict_word = pad_sequences(maxlen=max_len, sequences=predict_word, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "    predict_char = convert_word_to_char(predict_sent)\n",
    "    result_tag = model.predict([predict_word,np.array(predict_char).reshape((len(predict_char),max_len, max_len_char))])\n",
    "    p = np.argmax(result_tag, axis=-1)\n",
    "    pred=[i for i in p[0]]\n",
    "    revert_pred=[ix_to_ner[i] for i in p[0]]\n",
    "    \n",
    "    word=predict_sent\n",
    "    tag=revert_pred[:len_word]\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49733db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ทำงานดีกว่าเรียน <premise>เพราะได้</premise>ผลตอบแทน<premise>ถ้าเรียนผลแค่</premise>สอบผ่านยังไง<premise>ก็ทำงานอยู่ดีเมื่อจบ</premise>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ทำงานดีกว่าเรียน เพราะได้ผลตอบแทนถ้าเรียนผลแค่สอบผ่านยังไงก็ทำงานอยู่ดีเมื่อจบ'\n",
    "list_word, predict_tag = prediction(text)\n",
    "predict_tag = ['O' if tag == 'pad' else tag for tag in predict_tag]\n",
    "tag_html_format2(zip(list_word, predict_tag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
